# YOLOv3 ğŸš€ by Ultralytics, GPL-3.0 license
"""
Common modules
"""

import json
import math
import platform
import warnings
from copy import copy
from pathlib import Path

import cv2
import numpy as np
import pandas as pd
import requests
import torch
import torch.nn as nn
from PIL import Image
from torch.cuda import amp

from utils.datasets import exif_transpose, letterbox
from utils.general import (LOGGER, check_requirements, check_suffix, colorstr, increment_path, make_divisible,
                           non_max_suppression, scale_coords, xywh2xyxy, xyxy2xywh)
from utils.plots import Annotator, colors, save_one_box
from utils.torch_utils import time_sync


def autopad(k, p=None):  # kernel, padding
    """
            ç”¨äºConvå‡½æ•°å’ŒClassifyå‡½æ•°ä¸­,
            ä¸ºsameå·ç§¯æˆ–sameæ± åŒ–ä½œè‡ªåŠ¨æ‰©å……ï¼ˆ0å¡«å……ï¼‰  Pad to 'same'
            æ ¹æ®å·ç§¯æ ¸å¤§å°kè‡ªåŠ¨è®¡ç®—å·ç§¯æ ¸paddingæ•°ï¼ˆ0å¡«å……ï¼‰
            v3ä¸­åªæœ‰ä¸¤ç§å·ç§¯ï¼š
               1ã€ä¸‹é‡‡æ ·å·ç§¯:conv3x3 s=2 p=k//2=1
               2ã€feature sizeä¸å˜çš„å·ç§¯:conv1x1 s=1 p=k//2=1
            :params k: å·ç§¯æ ¸çš„kernel_size
            :return p: è‡ªåŠ¨è®¡ç®—çš„éœ€è¦padå€¼ï¼ˆ0å¡«å……ï¼‰
        """
    # Pad to 'same'
    if p is None:
        p = k // 2 if isinstance(k, int) else [x // 2 for x in k]  # auto-pad
    return p


class Conv(nn.Module):
    # Standard convolution
    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, act=True):  # ch_in, ch_out, kernel, stride, padding, groups
        """
               Standard convolution  conv+BN+act
               :params c1: è¾“å…¥çš„channelå€¼
               :params c2: è¾“å‡ºçš„channelå€¼
               :params k: å·ç§¯çš„kernel_size
               :params s: å·ç§¯çš„stride
               :params p: å·ç§¯çš„padding  ä¸€èˆ¬æ˜¯None  å¯ä»¥é€šè¿‡autopadè‡ªè¡Œè®¡ç®—éœ€è¦padçš„paddingæ•°
               :params g: å·ç§¯çš„groupsæ•°  =1å°±æ˜¯æ™®é€šçš„å·ç§¯  >1å°±æ˜¯æ·±åº¦å¯åˆ†ç¦»å·ç§¯,ä¹Ÿå°±æ˜¯åˆ†ç»„å·ç§¯
               :params act: æ¿€æ´»å‡½æ•°ç±»å‹   Trueå°±æ˜¯SiLU()/Swish   Falseå°±æ˜¯ä¸ä½¿ç”¨æ¿€æ´»å‡½æ•°
                            ç±»å‹æ˜¯nn.Moduleå°±ä½¿ç”¨ä¼ è¿›æ¥çš„æ¿€æ´»å‡½æ•°ç±»å‹
               """

        super().__init__()
        self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p), groups=g, bias=False)
        self.bn = nn.BatchNorm2d(c2)

        # Todo ä¿®æ”¹æ¿€æ´»å‡½æ•°
        # self.act = nn.Identity() if act is True else (act if isinstance(act, nn.Module) else nn.Identity())
        # self.act = nn.Tanh() if act is True else (act if isinstance(act, nn.Module) else nn.Identity())
        # self.act = nn.Sigmoid() if act is True else (act if isinstance(act, nn.Module) else nn.Identity())
        # self.act = nn.ReLU() if act is True else (act if isinstance(act, nn.Module) else nn.Identity())
        # self.act = nn.LeakyReLU(0.1) if act is True else (act if isinstance(act, nn.Module) else nn.Identity())
        # self.act = nn.Hardswish() if act is True else (act if isinstance(act, nn.Module) else nn.Identity())
        self.act = nn.SiLU() if act is True else (act if isinstance(act, nn.Module) else nn.Identity())

    def forward(self, x):
        # æ¨¡å‹çš„å‰å‘ä¼ æ’­
        return self.act(self.bn(self.conv(x)))

    def forward_fuse(self, x):
        """
            ç”¨äºModelç±»çš„fuseå‡½æ•°
            å‰å‘èåˆconv+bnè®¡ç®— åŠ é€Ÿæ¨ç† ä¸€èˆ¬ç”¨äºæµ‹è¯•/éªŒè¯é˜¶æ®µ
        """
        return self.act(self.conv(x))


class Bottleneck(nn.Module):
    # Standard bottleneck
    def __init__(self, c1, c2, shortcut=True, g=1, e=0.5):  # ch_in, ch_out, shortcut, groups, expansion

        """
               åœ¨yolo.pyçš„parse_modelä¸­è°ƒç”¨
               Standard bottleneck  Conv+Conv+shortcut

               :params c1: ç¬¬ä¸€ä¸ªå·ç§¯çš„è¾“å…¥channel
               :params c2: ç¬¬äºŒä¸ªå·ç§¯çš„è¾“å‡ºchannel
               :params shortcut: bool æ˜¯å¦æœ‰shortcutè¿æ¥ é»˜è®¤æ˜¯True
               :params g: å·ç§¯åˆ†ç»„çš„ä¸ªæ•°  =1å°±æ˜¯æ™®é€šå·ç§¯  >1å°±æ˜¯æ·±åº¦å¯åˆ†ç¦»å·ç§¯
               :params e: expansion ratio  e*c2å°±æ˜¯ç¬¬ä¸€ä¸ªå·ç§¯çš„è¾“å‡ºchannel=ç¬¬äºŒä¸ªå·ç§¯çš„è¾“å…¥channel
        """
        super().__init__()
        c_ = int(c2 * e)  # hidden channels
        self.cv1 = Conv(c1, c_, 1, 1)
        self.cv2 = Conv(c_, c2, 3, 1, g=g)
        self.add = shortcut and c1 == c2

    def forward(self, x):
        return x + self.cv2(self.cv1(x)) if self.add else self.cv2(self.cv1(x))


class SPP(nn.Module):
    """
            ç©ºé—´é‡‘å­—å¡”æ± åŒ– Spatial pyramid pooling layer used in YOLOv3-SPP
            :params c1: SPPæ¨¡å—çš„è¾“å…¥channel
            :params c2: SPPæ¨¡å—çš„è¾“å‡ºchannel
            :params k: ä¿å­˜ç€ä¸‰ä¸ªmaxpoolçš„å·ç§¯æ ¸å¤§å° é»˜è®¤æ˜¯(5, 9, 13)
    """
    # Spatial Pyramid Pooling (SPP) layer https://arxiv.org/abs/1406.4729
    def __init__(self, c1, c2, k=(5, 9, 13)):
        super().__init__()
        c_ = c1 // 2  # hidden channels
        self.cv1 = Conv(c1, c_, 1, 1)   # ç¬¬ä¸€å±‚å·ç§¯
        self.cv2 = Conv(c_ * (len(k) + 1), c2, 1, 1)   # æœ€åä¸€å±‚å·ç§¯  +1æ˜¯å› ä¸ºæœ‰len(k)+1ä¸ªè¾“å…¥
        self.m = nn.ModuleList([nn.MaxPool2d(kernel_size=x, stride=1, padding=x // 2) for x in k])

    def forward(self, x):
        x = self.cv1(x)
        with warnings.catch_warnings():
            warnings.simplefilter('ignore')  # suppress torch 1.9.0 max_pool2d() warning
            return self.cv2(torch.cat([x] + [m(x) for m in self.m], 1))


class Focus(nn.Module):
    r""" å°†å®½é«˜ä¿¡æ¯å‹ç¼©åˆ°é€šé“ç©ºé—´ä¸­ã€‚
    Focus å±‚é€šè¿‡å°†è¾“å…¥å›¾åƒçš„å››ä¸ªè±¡é™æ‹¼æ¥åœ¨ä¸€èµ·ï¼Œç„¶åé€šè¿‡ä¸€ä¸ªå·ç§¯å±‚æ¥æå–ç‰¹å¾ã€‚
    """

    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, act=True):  # è¾“å…¥é€šé“æ•°, è¾“å‡ºé€šé“æ•°, å·ç§¯æ ¸å¤§å°, æ­¥å¹…, å¡«å……, åˆ†ç»„æ•°, æ˜¯å¦ä½¿ç”¨æ¿€æ´»å‡½æ•°
        super().__init__()
        # åˆå§‹åŒ–å·ç§¯å±‚ï¼Œå°†å››ä¸ªè±¡é™æ‹¼æ¥åçš„ç‰¹å¾å›¾æ˜ å°„åˆ° c2 ä¸ªé€šé“
        self.conv = Conv(c1 * 4, c2, k, s, p, g, act)
        # æ³¨é‡Šæ‰äº†çš„ Contract å±‚ç”¨äºå°†å›¾åƒå°ºå¯¸å‡åŠ
        # self.contract = Contract(gain=2)

    def forward(self, x):  # x(b, c, w, h) -> y(b, 4c, w/2, h/2)
        # å°†è¾“å…¥ç‰¹å¾å›¾çš„å››ä¸ªè±¡é™æ‹¼æ¥èµ·æ¥ï¼Œå¹¶é€šè¿‡å·ç§¯å±‚è¿›è¡Œå¤„ç†
        return self.conv(torch.cat([
            x[..., ::2, ::2],  # å·¦ä¸Šè±¡é™
            x[..., 1::2, ::2],  # å³ä¸Šè±¡é™
            x[..., ::2, 1::2],  # å·¦ä¸‹è±¡é™
            x[..., 1::2, 1::2]   # å³ä¸‹è±¡é™
        ], 1))
        # ä½¿ç”¨ Contract å±‚ï¼ˆå¦‚æœå¯ç”¨ï¼‰å°†ç‰¹å¾å›¾å°ºå¯¸å‡åŠ
        # return self.conv(self.contract(x))


class Contract(nn.Module):
    """
        ç”¨åœ¨yolo.pyçš„parse_modelæ¨¡å—
        æ”¹å˜è¾“å…¥ç‰¹å¾çš„shape å°†wå’Œhç»´åº¦(ç¼©å°)çš„æ•°æ®æ”¶ç¼©åˆ°channelç»´åº¦ä¸Š(æ”¾å¤§)
        Contract width-height into channels, i.e. x(1,64,80,80) to x(1,256,40,40)
    """
    # Contract width-height into channels, i.e. x(1,64,80,80) to x(1,256,40,40)
    def __init__(self, gain=2):
        super().__init__()
        self.gain = gain

    def forward(self, x):
        b, c, h, w = x.size()  # assert (h / s == 0) and (W / s == 0), 'Indivisible gain'
        s = self.gain
        x = x.view(b, c, h // s, s, w // s, s)  # x(1,64,40,2,40,2)
        x = x.permute(0, 3, 5, 1, 2, 4).contiguous()  # x(1,2,2,64,40,40)
        return x.view(b, c * s * s, h // s, w // s)  # x(1,256,40,40)


class Expand(nn.Module):
    """
        ç”¨åœ¨yolo.pyçš„parse_modelæ¨¡å—  ç”¨çš„ä¸å¤š
        Expandå‡½æ•°ä¹Ÿæ˜¯æ”¹å˜è¾“å…¥ç‰¹å¾çš„shapeï¼Œä¸è¿‡ä¸Contractçš„ç›¸åï¼Œ æ˜¯å°†channelç»´åº¦(å˜å°)çš„æ•°æ®æ‰©å±•åˆ°Wå’ŒHç»´åº¦(å˜å¤§)ã€‚
        æ”¹å˜è¾“å…¥ç‰¹å¾çš„shape å°†channelç»´åº¦(å˜å°)çš„æ•°æ®æ‰©å±•åˆ°Wå’ŒHç»´åº¦(å˜å¤§)
        Expand channels into width-height, i.e. x(1,64,80,80) to x(1,16,160,160)
    """

    # Expand channels into width-height, i.e. x(1,64,80,80) to x(1,16,160,160)
    def __init__(self, gain=2):
        super().__init__()
        self.gain = gain

    def forward(self, x):
        b, c, h, w = x.size()  # assert C / s ** 2 == 0, 'Indivisible gain'
        s = self.gain
        x = x.view(b, s, s, c // s ** 2, h, w)  # x(1,2,2,16,80,80)
        x = x.permute(0, 3, 4, 1, 5, 2).contiguous()  # x(1,16,80,2,80,2)
        return x.view(b, c // s ** 2, h * s, w * s)  # x(1,16,160,160)


class Concat(nn.Module):
    # æŒ‰ç…§è‡ªèº«æŸä¸ªç»´åº¦è¿›è¡Œconcatï¼Œå¸¸ç”¨æ¥åˆå¹¶å‰åä¸¤ä¸ªfeature mapï¼Œä¹Ÿå°±æ˜¯ä¸Šé¢Yolo 5sç»“æ„å›¾ä¸­çš„Concatã€‚
    # Concatenate a list of tensors along dimension
    def __init__(self, dimension=1):
        super().__init__()
        self.d = dimension

    def forward(self, x):
        return torch.cat(x, self.d)


class DetectMultiBackend(nn.Module):
    # MultiBackend ç±»ç”¨äºåœ¨å„ç§åç«¯ä¸Šè¿›è¡Œ Python æ¨æ–­
    def __init__(self, weights='yolov3.pt', device=None, dnn=True):
        # ç”¨æ³•:
        #   PyTorch:      weights = *.pt
        #   TorchScript:            *.torchscript.pt
        #   CoreML:                 *.mlmodel
        #   TensorFlow:             *_saved_model
        #   TensorFlow:             *.pb
        #   TensorFlow Lite:        *.tflite
        #   ONNX Runtime:           *.onnx
        #   OpenCV DNN:             *.onnx with dnn=True
        super().__init__()
        w = str(weights[0] if isinstance(weights, list) else weights)  # å¤„ç†æƒé‡è·¯å¾„
        suffix, suffixes = Path(w).suffix.lower(), ['.pt', '.onnx', '.tflite', '.pb', '', '.mlmodel']
        check_suffix(w, suffixes)  # æ£€æŸ¥æƒé‡åç¼€æ˜¯å¦åœ¨å…è®¸çš„åˆ—è¡¨ä¸­
        pt, onnx, tflite, pb, saved_model, coreml = (suffix == x for x in suffixes)  # åç«¯å¸ƒå°”å€¼
        jit = pt and 'torchscript' in w.lower()  # åˆ¤æ–­æ˜¯å¦ä¸º TorchScript
        stride, names = 64, [f'class{i}' for i in range(1000)]  # è®¾ç½®é»˜è®¤æ­¥å¹…å’Œç±»åˆ«åç§°

        if jit:  # TorchScript
            LOGGER.info(f'Loading {w} for TorchScript inference...')  # æ—¥å¿—è®°å½•ï¼šåŠ è½½ TorchScript æ¨¡å‹
            extra_files = {'config.txt': ''}  # æ¨¡å‹å…ƒæ•°æ®
            model = torch.jit.load(w, _extra_files=extra_files)  # åŠ è½½ TorchScript æ¨¡å‹
            if extra_files['config.txt']:
                d = json.loads(extra_files['config.txt'])  # è§£æé¢å¤–çš„é…ç½®æ–‡ä»¶
                stride, names = int(d['stride']), d['names']  # æå–æ­¥å¹…å’Œç±»åˆ«åç§°

        elif pt:  # PyTorch
            from models.experimental import attempt_load  # å¯¼å…¥ attempt_load å‡½æ•°ï¼Œé¿å…å¾ªç¯å¯¼å…¥
            model = torch.jit.load(w) if 'torchscript' in w else attempt_load(weights, map_location=device)  # åŠ è½½ TorchScript æ¨¡å‹æˆ– PyTorch æ¨¡å‹
            stride = int(model.stride.max())  # è·å–æ¨¡å‹çš„æœ€å¤§æ­¥å¹…
            names = model.module.names if hasattr(model, 'module') else model.names  # è·å–ç±»åˆ«åç§°ï¼Œå¦‚æœæ¨¡å‹æœ‰ 'module' å±æ€§åˆ™ä»ä¸­è·å–ç±»åˆ«åç§°
        elif coreml:  # CoreML *.mlmodel
            import coremltools as ct  # å¯¼å…¥ CoreML å·¥å…·åŒ…
            model = ct.models.MLModel(w)  # åŠ è½½ CoreML æ¨¡å‹
        elif dnn:  # ONNX OpenCV DNN
            LOGGER.info(f'Loading {w} for ONNX OpenCV DNN inference...')  # æ—¥å¿—ï¼šåŠ è½½ ONNX æ¨¡å‹ç”¨äº OpenCV DNN æ¨ç†
            check_requirements(('opencv-python>=4.5.4',))  # æ£€æŸ¥æ˜¯å¦å®‰è£…äº† opencv-python åº“
            net = cv2.dnn.readNetFromONNX(w)  # ä½¿ç”¨ OpenCV DNN è¯»å– ONNX æ¨¡å‹
        elif onnx:  # ONNX Runtime
            LOGGER.info(f'Loading {w} for ONNX Runtime inference...')  # æ—¥å¿—ï¼šåŠ è½½ ONNX æ¨¡å‹ç”¨äº ONNX Runtime æ¨ç†
            check_requirements(('onnx', 'onnxruntime-gpu' if torch.has_cuda else 'onnxruntime'))  # æ£€æŸ¥æ˜¯å¦å®‰è£…äº† onnx å’Œ onnxruntime åº“
            import onnxruntime  # å¯¼å…¥ ONNX Runtime
            session = onnxruntime.InferenceSession(w, None)  # åˆ›å»º ONNX Runtime æ¨ç†ä¼šè¯
        else:  # TensorFlow æ¨¡å‹ (TFLite, pb, saved_model)
            # import tensorflow as tf  # å¯¼å…¥ TensorFlow åº“
            # if pb:  # å¦‚æœæ˜¯ TensorFlow Frozen Graph (.pb æ–‡ä»¶)
            #     def wrap_frozen_graph(gd, inputs, outputs):
            #         # åŒ…è£…å†»ç»“å›¾
            #         x = tf.compat.v1.wrap_function(lambda: tf.compat.v1.import_graph_def(gd, name=""), [])  # åŒ…è£…å‡½æ•°
            #         # å‰ªææ“ä½œï¼Œè·å–æŒ‡å®šè¾“å…¥å’Œè¾“å‡º
            #         return x.prune(tf.nest.map_structure(x.graph.as_graph_element, inputs),
            #                        tf.nest.map_structure(x.graph.as_graph_element, outputs))
            #
            #     LOGGER.info(f'Loading {w} for TensorFlow *.pb inference...')  # æ—¥å¿—ï¼šåŠ è½½ TensorFlow .pb æ¨¡å‹è¿›è¡Œæ¨ç†
            #     graph_def = tf.Graph().as_graph_def()  # åˆ›å»ºä¸€ä¸ªæ–°çš„å›¾å½¢å®šä¹‰
            #     graph_def.ParseFromString(open(w, 'rb').read())  # ä» .pb æ–‡ä»¶è¯»å–å›¾å½¢å®šä¹‰
            #     # åŒ…è£…å†»ç»“å›¾å‡½æ•°ï¼ŒæŒ‡å®šè¾“å…¥å’Œè¾“å‡º
            #     frozen_func = wrap_frozen_graph(gd=graph_def, inputs="x:0", outputs="Identity:0")
            #
            # elif saved_model:  # å¦‚æœæ˜¯ TensorFlow SavedModel
            #     LOGGER.info(f'Loading {w} for TensorFlow saved_model inference...')  # æ—¥å¿—ï¼šåŠ è½½ TensorFlow SavedModel è¿›è¡Œæ¨ç†
            #     model = tf.keras.models.load_model(w)  # åŠ è½½ SavedModel
            #
            # elif tflite:  # å¦‚æœæ˜¯ TensorFlow Lite æ¨¡å‹
            #     if 'edgetpu' in w.lower():  # å¦‚æœæ˜¯ Edge TPU æ¨¡å‹
            #         LOGGER.info(f'Loading {w} for TensorFlow Edge TPU inference...')  # æ—¥å¿—ï¼šåŠ è½½ TensorFlow Edge TPU æ¨¡å‹è¿›è¡Œæ¨ç†
            #         import tflite_runtime.interpreter as tfli  # å¯¼å…¥ tflite_runtime
            #         # æ ¹æ®å¹³å°é€‰æ‹©ç›¸åº”çš„ Edge TPU åŠ¨æ€åº“
            #         delegate = {'Linux': 'libedgetpu.so.1',  # Linux ä¸‹çš„åº“
            #                     'Darwin': 'libedgetpu.1.dylib',  # macOS ä¸‹çš„åº“
            #                     'Windows': 'edgetpu.dll'}[platform.system()]  # Windows ä¸‹çš„åº“
            #         # åˆ›å»º Edge TPU è§£é‡Šå™¨
            #         interpreter = tfli.Interpreter(model_path=w, experimental_delegates=[tfli.load_delegate(delegate)])
            #     else:
            #         LOGGER.info(f'Loading {w} for TensorFlow Lite inference...')  # æ—¥å¿—ï¼šåŠ è½½ TensorFlow Lite æ¨¡å‹è¿›è¡Œæ¨ç†
            #         interpreter = tf.lite.Interpreter(model_path=w)  # åŠ è½½ TFLite æ¨¡å‹
            #     interpreter.allocate_tensors()  # åˆ†é…å¼ é‡
            #     input_details = interpreter.get_input_details()  # è·å–è¾“å…¥ç»†èŠ‚
            #     output_details = interpreter.get_output_details()  # è·å–è¾“å‡ºç»†èŠ‚
            import tensorflow as tf
            if pb:  # https://www.tensorflow.org/guide/migrate#a_graphpb_or_graphpbtxt
                def wrap_frozen_graph(gd, inputs, outputs):
                    x = tf.compat.v1.wrap_function(lambda: tf.compat.v1.import_graph_def(gd, name=""), [])  # wrapped
                    return x.prune(tf.nest.map_structure(x.graph.as_graph_element, inputs),
                                   tf.nest.map_structure(x.graph.as_graph_element, outputs))

                LOGGER.info(f'Loading {w} for TensorFlow *.pb inference...')
                graph_def = tf.Graph().as_graph_def()
                graph_def.ParseFromString(open(w, 'rb').read())
                frozen_func = wrap_frozen_graph(gd=graph_def, inputs="x:0", outputs="Identity:0")
            elif saved_model:
                LOGGER.info(f'Loading {w} for TensorFlow saved_model inference...')
                model = tf.keras.models.load_model(w)
            elif tflite:  # https://www.tensorflow.org/lite/guide/python#install_tensorflow_lite_for_python
                if 'edgetpu' in w.lower():
                    LOGGER.info(f'Loading {w} for TensorFlow Edge TPU inference...')
                    import tflite_runtime.interpreter as tfli
                    delegate = {'Linux': 'libedgetpu.so.1',  # install https://coral.ai/software/#edgetpu-runtime
                                'Darwin': 'libedgetpu.1.dylib',
                                'Windows': 'edgetpu.dll'}[platform.system()]
                    interpreter = tfli.Interpreter(model_path=w, experimental_delegates=[tfli.load_delegate(delegate)])
                else:
                    LOGGER.info(f'Loading {w} for TensorFlow Lite inference...')
                    interpreter = tf.lite.Interpreter(model_path=w)  # load TFLite model
                interpreter.allocate_tensors()  # allocate
                input_details = interpreter.get_input_details()  # inputs
                output_details = interpreter.get_output_details()  # outputs
        self.__dict__.update(locals())  # å°†æ‰€æœ‰å±€éƒ¨å˜é‡èµ‹å€¼ç»™å®ä¾‹å±æ€§

    def forward(self, im, augment=False, visualize=False, val=False):
        # MultiBackend æ¨ç†
        b, ch, h, w = im.shape  # æ‰¹é‡å¤§å°ã€é€šé“æ•°ã€é«˜åº¦ã€å®½åº¦

        if self.pt:  # PyTorch
            y = self.model(im) if self.jit else self.model(im, augment=augment, visualize=visualize)
            return y if val else y[0]

        elif self.coreml:  # CoreML *.mlmodel
            im = im.permute(0, 2, 3, 1).cpu().numpy()  # torch BCHW è½¬ä¸º numpy BHWC æ ¼å¼ shape(1,320,192,3)
            im = Image.fromarray((im[0] * 255).astype('uint8'))  # è½¬æ¢ä¸º PIL å›¾åƒ
            # im = im.resize((192, 320), Image.ANTIALIAS)  # ï¼ˆå¯é€‰ï¼‰è°ƒæ•´å›¾åƒå¤§å°
            y = self.model.predict({'image': im})  # ä½¿ç”¨ CoreML æ¨¡å‹è¿›è¡Œé¢„æµ‹ï¼Œè¿”å›çš„æ˜¯ xywh å½’ä¸€åŒ–åæ ‡
            box = xywh2xyxy(y['coordinates'] * [[w, h, w, h]])  # è½¬æ¢ä¸º xyxy åƒç´ åæ ‡
            conf, cls = y['confidence'].max(1), y['confidence'].argmax(1).astype(np.float)  # è·å–ç½®ä¿¡åº¦å’Œç±»åˆ«
            y = np.concatenate((box, conf.reshape(-1, 1), cls.reshape(-1, 1)), 1)  # åˆå¹¶ç»“æœ

        elif self.onnx:  # ONNX
            im = im.cpu().numpy()  # å°† torch å¼ é‡è½¬ä¸º numpy æ•°ç»„
            if self.dnn:  # ONNX OpenCV DNN
                self.net.setInput(im)  # è®¾ç½®è¾“å…¥
                y = self.net.forward()  # å‰å‘æ¨ç†
            else:  # ONNX Runtime
                y = self.session.run([self.session.get_outputs()[0].name], {self.session.get_inputs()[0].name: im})[
                    0]  # è¿è¡Œæ¨ç†

        else:  # TensorFlow æ¨¡å‹ï¼ˆTFLite, pb, saved_modelï¼‰
            im = im.permute(0, 2, 3, 1).cpu().numpy()  # torch BCHW è½¬ä¸º numpy BHWC æ ¼å¼ shape(1,320,192,3)
            if self.pb:  # TensorFlow Frozen Graph
                y = self.frozen_func(x=self.tf.constant(im)).numpy()  # æ‰§è¡Œæ¨ç†
            elif self.saved_model:  # TensorFlow SavedModel
                y = self.model(im, training=False).numpy()  # æ‰§è¡Œæ¨ç†
            elif self.tflite:  # TensorFlow Lite
                input, output = self.input_details[0], self.output_details[0]
                int8 = input['dtype'] == np.uint8  # æ˜¯å¦æ˜¯ TFLite é‡åŒ– uint8 æ¨¡å‹
                if int8:
                    scale, zero_point = input['quantization']
                    im = (im / scale + zero_point).astype(np.uint8)  # åé‡åŒ–
                self.interpreter.set_tensor(input['index'], im)  # è®¾ç½®è¾“å…¥å¼ é‡
                self.interpreter.invoke()  # æ‰§è¡Œæ¨ç†
                y = self.interpreter.get_tensor(output['index'])  # è·å–è¾“å‡ºå¼ é‡
                if int8:
                    scale, zero_point = output['quantization']
                    y = (y.astype(np.float32) - zero_point) * scale  # åé‡åŒ–
            y[..., 0] *= w  # è½¬æ¢ x æ–¹å‘åæ ‡
            y[..., 1] *= h  # è½¬æ¢ y æ–¹å‘åæ ‡
            y[..., 2] *= w  # è½¬æ¢å®½åº¦
            y[..., 3] *= h  # è½¬æ¢é«˜åº¦
        y = torch.tensor(y)  # å°†ç»“æœè½¬ä¸º torch å¼ é‡
        return (y, []) if val else y  # å¦‚æœ val ä¸º Trueï¼Œåˆ™è¿”å› (y, [])ï¼Œå¦åˆ™åªè¿”å› y


class AutoShape(nn.Module):
    # è¾“å…¥å¥å£®çš„æ¨¡å‹åŒ…è£…å™¨ï¼Œç”¨äºå¤„ç† cv2/np/PIL/torch è¾“å…¥ã€‚åŒ…æ‹¬é¢„å¤„ç†ã€æ¨ç†å’Œ NMSï¼ˆéæå¤§å€¼æŠ‘åˆ¶ï¼‰
    conf = 0.25  # NMS ç½®ä¿¡åº¦é˜ˆå€¼
    iou = 0.45  # NMS IoU é˜ˆå€¼
    classes = None  # ï¼ˆå¯é€‰åˆ—è¡¨ï¼‰æŒ‰ç±»åˆ«è¿‡æ»¤ï¼Œä¾‹å¦‚ COCO ä¸­çš„äººã€çŒ«å’Œç‹— = [0, 15, 16]
    multi_label = False  # NMS å…è®¸æ¯ä¸ªæ¡†æœ‰å¤šä¸ªæ ‡ç­¾
    max_det = 1000  # æ¯å¼ å›¾ç‰‡çš„æœ€å¤§æ£€æµ‹æ•°é‡

    def __init__(self, model):
        super().__init__()
        self.model = model.eval()  # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼

    def autoshape(self):
        LOGGER.info('AutoShape already enabled, skipping... ')  # æ¨¡å‹å·²è½¬æ¢ä¸º model.autoshape()ï¼Œè·³è¿‡
        return self

    def _apply(self, fn):
        # å¯¹æ¨¡å‹ä¸­éå‚æ•°æˆ–æœªæ³¨å†Œçš„ç¼“å†²åŒºçš„å¼ é‡åº”ç”¨ to()ã€cpu()ã€cuda()ã€half() ç­‰æ–¹æ³•
        self = super()._apply(fn)
        m = self.model.model[-1]  # Detect()
        m.stride = fn(m.stride)  # åº”ç”¨å‡½æ•°åˆ° stride
        m.grid = list(map(fn, m.grid))  # åº”ç”¨å‡½æ•°åˆ° grid åˆ—è¡¨
        if isinstance(m.anchor_grid, list):
            m.anchor_grid = list(map(fn, m.anchor_grid))  # åº”ç”¨å‡½æ•°åˆ° anchor_grid åˆ—è¡¨
        return self

    @torch.no_grad()
    def forward(self, imgs, size=640, augment=False, profile=False):
        # ä»å„ç§è¾“å…¥æºè¿›è¡Œæ¨æ–­ã€‚å¯¹äº height=640ï¼Œwidth=1280ï¼ŒRGB å›¾åƒçš„ç¤ºä¾‹è¾“å…¥å¦‚ä¸‹ï¼š
        #   file:       imgs = 'data/images/zidane.jpg'  # str æˆ– PosixPath
        #   URI:             = 'https://ultralytics.com/images/zidane.jpg'
        #   OpenCV:          = cv2.imread('image.jpg')[:,:,::-1]  # HWC BGR è½¬ RGB x(640,1280,3)
        #   PIL:             = Image.open('image.jpg') æˆ– ImageGrab.grab()  # HWC x(640,1280,3)
        #   numpy:           = np.zeros((640,1280,3))  # HWC
        #   torch:           = torch.zeros(16,3,320,640)  # BCHW (ç¼©æ”¾åˆ° size=640ï¼Œ0-1 å€¼)
        #   multiple:        = [Image.open('image1.jpg'), Image.open('image2.jpg'), ...]  # å›¾åƒåˆ—è¡¨

        t = [time_sync()]  # è®°å½•å¼€å§‹æ—¶é—´
        p = next(self.model.parameters())  # è·å–æ¨¡å‹çš„è®¾å¤‡å’Œæ•°æ®ç±»å‹
        if isinstance(imgs, torch.Tensor):  # å¦‚æœè¾“å…¥æ˜¯ torch.Tensor
            with amp.autocast(enabled=p.device.type != 'cpu'):  # è‡ªåŠ¨æ··åˆç²¾åº¦
                return self.model(imgs.to(p.device).type_as(p), augment, profile)  # æ¨æ–­

        # é¢„å¤„ç†
        n, imgs = (len(imgs), imgs) if isinstance(imgs, list) else (1, [imgs])  # å›¾åƒæ•°é‡å’Œå›¾åƒåˆ—è¡¨
        shape0, shape1, files = [], [], []  # å›¾åƒå’Œæ¨æ–­å½¢çŠ¶ï¼Œæ–‡ä»¶å
        for i, im in enumerate(imgs):
            f = f'image{i}'  # æ–‡ä»¶å
            if isinstance(im, (str, Path)):  # æ–‡ä»¶åæˆ– URI
                im, f = Image.open(requests.get(im, stream=True).raw if str(im).startswith('http') else im), im
                im = np.asarray(exif_transpose(im))
            elif isinstance(im, Image.Image):  # PIL Image
                im, f = np.asarray(exif_transpose(im)), getattr(im, 'filename', f) or f
            files.append(Path(f).with_suffix('.jpg').name)
            if im.shape[0] < 5:  # å›¾åƒåœ¨ CHW æ ¼å¼
                im = im.transpose((1, 2, 0))  # åè½¬ dataloader .transpose(2, 0, 1)
            im = im[..., :3] if im.ndim == 3 else np.tile(im[..., None], 3)  # å¼ºåˆ¶ä¸‰é€šé“è¾“å…¥
            s = im.shape[:2]  # HWC
            shape0.append(s)  # å›¾åƒå½¢çŠ¶
            g = (size / max(s))  # ç¼©æ”¾å› å­
            shape1.append([y * g for y in s])
            imgs[i] = im if im.data.contiguous else np.ascontiguousarray(im)  # æ›´æ–°
        shape1 = [make_divisible(x, int(self.stride.max())) for x in np.stack(shape1, 0).max(0)]  # æ¨æ–­å½¢çŠ¶
        x = [letterbox(im, new_shape=shape1, auto=False)[0] for im in imgs]  # å¡«å……
        x = np.stack(x, 0) if n > 1 else x[0][None]  # å †å 
        x = np.ascontiguousarray(x.transpose((0, 3, 1, 2)))  # BHWC è½¬ BCHW
        x = torch.from_numpy(x).to(p.device).type_as(p) / 255  # uint8 è½¬ fp16/32
        t.append(time_sync())  # è®°å½•æ—¶é—´

        with amp.autocast(enabled=p.device.type != 'cpu'):  # è‡ªåŠ¨æ··åˆç²¾åº¦
            # æ¨æ–­
            y = self.model(x, augment, profile)[0]  # å‰å‘ä¼ æ’­
            t.append(time_sync())  # è®°å½•æ—¶é—´

            # åå¤„ç†
            y = non_max_suppression(y, self.conf, iou_thres=self.iou, classes=self.classes,
                                    multi_label=self.multi_label, max_det=self.max_det)  # NMS
            for i in range(n):
                scale_coords(shape1, y[i][:, :4], shape0[i])  # ç¼©æ”¾åæ ‡

            t.append(time_sync())  # è®°å½•æ—¶é—´
            return Detections(imgs, y, files, t, self.names, x.shape)  # è¿”å›æ£€æµ‹ç»“æœ


class Detections:
    r""" ç”¨äºæ¨ç†ç»“æœçš„æ£€æµ‹ç±»ã€‚
    æ­¤ç±»ç”¨äºå¤„ç†æ¨¡å‹çš„æ¨ç†è¾“å‡ºï¼ŒåŒ…æ‹¬å›¾åƒã€é¢„æµ‹æ¡†ã€æ–‡ä»¶åç­‰ä¿¡æ¯ï¼Œå¹¶æä¾›å½’ä¸€åŒ–åçš„æ¡†åæ ‡ã€‚
    """
    def __init__(self, imgs, pred, files, times=None, names=None, shape=None):
        super().__init__()
        d = pred[0].device  # è·å–è®¾å¤‡ç±»å‹ï¼ˆCPUæˆ–GPUï¼‰

        # è®¡ç®—æ¯å¼ å›¾åƒçš„å½’ä¸€åŒ–å› å­
        gn = [torch.tensor([*(im.shape[i] for i in [1, 0, 1, 0]), 1, 1], device=d) for im in imgs]  # å½’ä¸€åŒ–å› å­

        # åˆå§‹åŒ–ç±»å±æ€§
        self.imgs = imgs  # å›¾åƒåˆ—è¡¨ï¼Œä½œä¸º numpy æ•°ç»„
        self.pred = pred  # é¢„æµ‹ç»“æœåˆ—è¡¨ï¼Œpred[0] åŒ…å« (xyxy, conf, cls) ä¿¡æ¯
        self.names = names  # ç±»åˆ«åç§°
        self.files = files  # å›¾åƒæ–‡ä»¶ååˆ—è¡¨
        self.xyxy = pred  # xyxy åƒç´ åæ ‡
        self.xywh = [xyxy2xywh(x) for x in pred]  # xywh åƒç´ åæ ‡
        self.xyxyn = [x / g for x, g in zip(self.xyxy, gn)]  # xyxy å½’ä¸€åŒ–åæ ‡
        self.xywhn = [x / g for x, g in zip(self.xywh, gn)]  # xywh å½’ä¸€åŒ–åæ ‡
        self.n = len(self.pred)  # å›¾åƒæ•°é‡ï¼ˆæ‰¹æ¬¡å¤§å°ï¼‰
        self.t = tuple((times[i + 1] - times[i]) * 1000 / self.n for i in range(3))  # æ—¶é—´æˆ³ï¼ˆæ¯«ç§’ï¼‰
        self.s = shape  # æ¨ç†æ—¶çš„ BCHW å½¢çŠ¶

    def display(self, pprint=False, show=False, save=False, crop=False, render=False, save_dir=Path('')):
        r""" æ˜¾ç¤ºã€ä¿å­˜æˆ–è£å‰ªæ£€æµ‹ç»“æœã€‚

        æ ¹æ®è®¾ç½®ï¼Œæ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š
        - `pprint`: æ‰“å°æ£€æµ‹ä¿¡æ¯åˆ°æ—¥å¿—ã€‚
        - `show`: æ˜¾ç¤ºæ£€æµ‹ç»“æœå›¾åƒã€‚
        - `save`: ä¿å­˜æ£€æµ‹ç»“æœå›¾åƒåˆ°æŒ‡å®šç›®å½•ã€‚
        - `crop`: è£å‰ªæ£€æµ‹åŒºåŸŸå¹¶ä¿å­˜ã€‚
        - `render`: æ¸²æŸ“æ£€æµ‹ç»“æœåˆ°å›¾åƒåˆ—è¡¨ä¸­ã€‚

        å‚æ•°ï¼š
        - `save_dir`: ä¿å­˜å›¾åƒçš„ç›®å½•è·¯å¾„ã€‚
        """
        crops = []  # å­˜å‚¨è£å‰ªçš„æ£€æµ‹åŒºåŸŸ
        for i, (im, pred) in enumerate(zip(self.imgs, self.pred)):
            s = f'image {i + 1}/{len(self.pred)}: {im.shape[0]}x{im.shape[1]} '  # ç”Ÿæˆå›¾åƒä¿¡æ¯å­—ç¬¦ä¸²
            if pred.shape[0]:
                for c in pred[:, -1].unique():
                    n = (pred[:, -1] == c).sum()  # æ¯ä¸ªç±»åˆ«çš„æ£€æµ‹æ•°é‡
                    s += f"{n} {self.names[int(c)]}{'s' * (n > 1)}, "  # å°†ç±»åˆ«å’Œæ•°é‡æ·»åŠ åˆ°ä¿¡æ¯å­—ç¬¦ä¸²
                if show or save or render or crop:
                    annotator = Annotator(im, example=str(self.names))  # åˆå§‹åŒ–æ³¨é‡Šå·¥å…·
                    for *box, conf, cls in reversed(pred):  # é€†åºéå†é¢„æµ‹æ¡†
                        label = f'{self.names[int(cls)]} {conf:.2f}'  # æ ‡ç­¾
                        if crop:
                            file = save_dir / 'crops' / self.names[int(cls)] / self.files[i] if save else None
                            crops.append({'box': box, 'conf': conf, 'cls': cls, 'label': label,
                                          'im': save_one_box(box, im, file=file, save=save)})
                        else:  # å…¶ä»–æ“ä½œ
                            annotator.box_label(box, label, color=colors(cls))
                    im = annotator.im  # æ›´æ–°å›¾åƒ
            else:
                s += '(no detections)'  # æ²¡æœ‰æ£€æµ‹åˆ°ç›®æ ‡

            im = Image.fromarray(im.astype(np.uint8)) if isinstance(im, np.ndarray) else im  # å¦‚æœå›¾åƒæ˜¯ numpy æ•°ç»„ï¼Œåˆ™è½¬æ¢ä¸º PIL å›¾åƒ
            if pprint:
                LOGGER.info(s.rstrip(', '))  # æ‰“å°ä¿¡æ¯
            if show:
                im.show(self.files[i])  # æ˜¾ç¤ºå›¾åƒ
            if save:
                f = self.files[i]
                im.save(save_dir / f)  # ä¿å­˜å›¾åƒ
                if i == self.n - 1:
                    LOGGER.info(f"Saved {self.n} image{'s' * (self.n > 1)} to {colorstr('bold', save_dir)}")
            if render:
                self.imgs[i] = np.asarray(im)  # æ¸²æŸ“å›¾åƒ
        if crop:
            if save:
                LOGGER.info(f'Saved results to {save_dir}\n')
            return crops

    def print(self):
        r""" æ‰“å°æ£€æµ‹ç»“æœå’Œå¤„ç†é€Ÿåº¦ä¿¡æ¯ã€‚"""
        self.display(pprint=True)  # æ‰“å°æ£€æµ‹ç»“æœ
        LOGGER.info(f'Speed: %.1fms pre-process, %.1fms inference, %.1fms NMS per image at shape {tuple(self.s)}' %
                    self.t)  # æ‰“å°æ¯å¼ å›¾åƒçš„å¤„ç†é€Ÿåº¦ï¼ˆé¢„å¤„ç†ã€æ¨ç†ã€éæå¤§å€¼æŠ‘åˆ¶ï¼‰ä»¥åŠå›¾åƒçš„å½¢çŠ¶
        # print(f'Speed: %.1fms pre-process, %.1fms inference, %.1fms NMS per image at shape {tuple(self.s)}' %
        #             self.t)  # æ‰“å°æ¯å¼ å›¾åƒçš„å¤„ç†é€Ÿåº¦ï¼ˆé¢„å¤„ç†ã€æ¨ç†ã€éæå¤§å€¼æŠ‘åˆ¶ï¼‰ä»¥åŠå›¾åƒçš„å½¢çŠ¶

    def show(self):
        r""" æ˜¾ç¤ºæ£€æµ‹ç»“æœã€‚"""
        self.display(show=True)  # æ˜¾ç¤ºæ£€æµ‹ç»“æœ

    def save(self, save_dir='runs/detect/exp'):
        r""" ä¿å­˜æ£€æµ‹ç»“æœåˆ°æŒ‡å®šç›®å½•ã€‚"""
        save_dir = increment_path(save_dir, exist_ok=save_dir != 'runs/detect/exp', mkdir=True)  # é€’å¢ç›®å½•åç§°
        self.display(save=True, save_dir=save_dir)  # ä¿å­˜ç»“æœ

    def crop(self, save=True, save_dir='runs/detect/exp'):
        r""" è£å‰ªæ£€æµ‹ç»“æœå¹¶ä¿å­˜ã€‚"""
        save_dir = increment_path(save_dir, exist_ok=save_dir != 'runs/detect/exp', mkdir=True) if save else None
        return self.display(crop=True, save=save, save_dir=save_dir)  # è£å‰ªç»“æœ

    def render(self):
        r""" æ¸²æŸ“æ£€æµ‹ç»“æœã€‚"""
        self.display(render=True)  # æ¸²æŸ“ç»“æœ
        return self.imgs  # è¿”å›æ¸²æŸ“åçš„å›¾åƒ

    def pandas(self):
        r""" å°†æ£€æµ‹ç»“æœè½¬æ¢ä¸º pandas DataFrame æ ¼å¼ã€‚"""
        new = copy(self)  # è¿”å›å¯¹è±¡çš„å‰¯æœ¬
        ca = 'xmin', 'ymin', 'xmax', 'ymax', 'confidence', 'class', 'name'  # xyxy åˆ—å
        cb = 'xcenter', 'ycenter', 'width', 'height', 'confidence', 'class', 'name'  # xywh åˆ—å

        # éå† 'xyxy', 'xyxyn', 'xywh', 'xywhn' å­—æ®µåŠå…¶å¯¹åº”çš„åˆ—å
        for k, c in zip(['xyxy', 'xyxyn', 'xywh', 'xywhn'], [ca, ca, cb, cb]):
            # å°†æ£€æµ‹ç»“æœè½¬æ¢ä¸º DataFrameï¼Œå¹¶æ›´æ–°åˆ—å
            a = [[x[:5] + [int(x[5]), self.names[int(x[5])]] for x in x.tolist()] for x in getattr(self, k)]
            setattr(new, k, [pd.DataFrame(x, columns=c) for x in a])  # è®¾ç½® DataFrame å±æ€§

        return new  # è¿”å›åŒ…å« DataFrame çš„å‰¯æœ¬å¯¹è±¡

    def tolist(self):
        r""" è¿”å›ä¸€ä¸ª Detections å¯¹è±¡çš„åˆ—è¡¨ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥ç”¨ 'for result in results.tolist():' éå†ã€‚"""
        # åˆ›å»ºä¸€ä¸ª Detections å¯¹è±¡çš„åˆ—è¡¨ï¼Œæ¯ä¸ªå¯¹è±¡åŒ…å«ä¸€ä¸ªå›¾åƒå’Œå¯¹åº”çš„é¢„æµ‹ç»“æœ
        x = [Detections([self.imgs[i]], [self.pred[i]], self.names, self.s) for i in range(self.n)]
        # å¯¹æ¯ä¸ª Detections å¯¹è±¡ï¼Œç§»é™¤å…¶å†…éƒ¨åˆ—è¡¨ï¼Œä½¿å…¶å±æ€§ä¸ºå•ä¸€å…ƒç´ 
        for d in x:
            for k in ['imgs', 'pred', 'xyxy', 'xyxyn', 'xywh', 'xywhn']:
                setattr(d, k, getattr(d, k)[0])  # ä»åˆ—è¡¨ä¸­å¼¹å‡º
        return x  # è¿”å›åŒ…å« Detections å¯¹è±¡çš„åˆ—è¡¨

    def __len__(self):
        r""" è¿”å› Detections å¯¹è±¡ä¸­å›¾åƒçš„æ•°é‡ã€‚"""
        return self.n


class Classify(nn.Module):
    # åˆ†ç±»å¤´ï¼Œå°†è¾“å…¥ x(b,c1,20,20) è½¬æ¢ä¸º x(b,c2)
    def __init__(self, c1, c2, k=1, s=1, p=None, g=1):  # è¾“å…¥é€šé“æ•°, è¾“å‡ºé€šé“æ•°, å·ç§¯æ ¸å¤§å°, æ­¥å¹…, å¡«å……, åˆ†ç»„
        super().__init__()
        self.aap = nn.AdaptiveAvgPool2d(1)  # è‡ªé€‚åº”å¹³å‡æ± åŒ–åˆ° x(b,c1,1,1)
        self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p), groups=g)  # å·ç§¯å±‚ï¼Œå°† x è½¬æ¢ä¸º x(b,c2,1,1)
        self.flat = nn.Flatten()  # å±•å¹³å±‚

    def forward(self, x):
        # å¦‚æœ x æ˜¯åˆ—è¡¨ï¼Œåˆ™å¯¹åˆ—è¡¨ä¸­çš„æ¯ä¸ªå…ƒç´ è¿›è¡Œæ± åŒ–ï¼Œå¹¶æ‹¼æ¥æˆä¸€ä¸ª tensor
        z = torch.cat([self.aap(y) for y in (x if isinstance(x, list) else [x])], 1)
        return self.flat(self.conv(z))  # å·ç§¯åå±•å¹³ä¸º x(b,c2)

